{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMweu+cgQhcccLl5x/f/S3s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abdussahid26/Dara-preparation-and-sampling-for-LLMs/blob/main/Data_Sampling_with_a_Sliding_Window_Approach.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In this file we are going to implement a dataloader that fetches the input-target pairs from the training dataset using a sliding window approach.**\n",
        "\n"
      ],
      "metadata": {
        "id": "U3S6zwdiVTNQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMKAie60TmBm",
        "outputId": "514944e7-ca6e-40f1-f027-506fb19ce950"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.12.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6aqFmFF0TLqF",
        "outputId": "639785c8-8f8e-49ee-cf11-2febf615f32b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch version: 2.5.1+cu121\n",
            "tiktoken version: 0.8.0\n"
          ]
        }
      ],
      "source": [
        "from importlib.metadata import version\n",
        "\n",
        "print(\"torch version:\", version(\"torch\"))\n",
        "print(\"tiktoken version:\", version(\"tiktoken\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib.request\n",
        "\n",
        "url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
        "       \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
        "       \"the-verdict.txt\")\n",
        "file_path = \"the-verdict.txt\"\n",
        "urllib.request.urlretrieve(url, file_path)\n",
        "\n",
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()"
      ],
      "metadata": {
        "id": "bprGJd1TTehd"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenizing the whole 'The Verdict' short story using the **byte pair encoding (BPE)** tokenizer."
      ],
      "metadata": {
        "id": "xmVdLL5HT_Ou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ],
      "metadata": {
        "id": "x8x4BEVpUK5I"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "enc_text = tokenizer.encode(raw_text)\n",
        "print(\"Total number of tokens in the training set after applying the BPE tokenizer: \", len(enc_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "087w_AUjTtMD",
        "outputId": "861850d7-604c-4aeb-bff1-a10a7354ad54"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of tokens in the training set after applying the BPE tokenizer:  5145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, remove the first 50 tokens from the dataset for demonstration purposes."
      ],
      "metadata": {
        "id": "cAfI3lUtWooL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "enc_sample = enc_text[50:]\n",
        "\n",
        "context_size = 4 # The context size determines how many tokens are included in the input.\n",
        "x = enc_sample[:context_size]\n",
        "y = enc_sample[1:context_size+1]\n",
        "\n",
        "print(f\"x: {x}\")\n",
        "print(f\"y:      {y}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUYlaaHHT1BS",
        "outputId": "5f39562f-98f0-4891-fd69-960d99ebfdb2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x: [290, 4920, 2241, 287]\n",
            "y:      [4920, 2241, 287, 257]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By processing the inputs along with the targets, which are the inputs shifted by one position, we can create the next-word prediction tasks."
      ],
      "metadata": {
        "id": "jZ8_mI7pYu-p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Following are the input-output pairs that we can use for LLM training:\")\n",
        "\n",
        "for i in range(1, context_size+1):\n",
        "    context = enc_sample[:i]\n",
        "    target = enc_sample[i]\n",
        "    print(context, \"--->\", target)\n",
        "    print(tokenizer.decode(context), \"--->\", tokenizer.decode([target]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SuvYMgYFZun3",
        "outputId": "38e4799a-c866-4d21-d2dd-b6cc09017bf8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Following are the input-output pairs that we can use for LLM training:\n",
            "[290] ---> 4920\n",
            " and --->  established\n",
            "[290, 4920] ---> 2241\n",
            " and established --->  himself\n",
            "[290, 4920, 2241] ---> 287\n",
            " and established himself --->  in\n",
            "[290, 4920, 2241, 287] ---> 257\n",
            " and established himself in --->  a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **The efficient dataloader implementation.**\n",
        "\n",
        "#### Step 1: Tokenize the entire text.\n",
        "#### Step 2: Use a sliding window to chunk the book into overlapping sequences of max_length.\n",
        "#### Step 3: Return the total number of rows in the dataset.\n",
        "#### Step 4: Return a single row from the dataset."
      ],
      "metadata": {
        "id": "FaBQctXOcjBq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class GPTDatasetV1(Dataset):\n",
        "    def __init__(self, txt, tokenizer, max_length, stride): # max_length means context_size;  the stride determines how much we slide during applying sliding window approach\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        token_ids = tokenizer.encode(txt) # Tokenizes the entire text\n",
        "\n",
        "        for i in range(0, len(token_ids) - max_length, stride): # Uses a sliding window approach to chunk the book into overlapping sequences of max_length\n",
        "            input_chunk = token_ids[i:i + max_length]\n",
        "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "    def __len__(self): # Returns the total number of rows in the dataset\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx): # Returns a single row from the dataset\n",
        "        return self.input_ids[idx], self.target_ids[idx]"
      ],
      "metadata": {
        "id": "l9bytucDdZer"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code uses the GPTDatasetV1 to load the inputs in batches via a PyTorch DataLoader.\n",
        "\n",
        "#### Step 1: Initialize the tokenizer.\n",
        "#### Step 2: Create dataset.\n",
        "#### Step 3: drop_last = True; drops the last batch if it is shorter than the specified batch_size to prevent loss spikes during training.\n",
        "#### Step 4: The number of CPU processes to use for preprocessing.\n",
        "\n",
        "Why do we do this dataloader? Because it helps us to do parallel processing and it also analyzes multiple batches at one time."
      ],
      "metadata": {
        "id": "HIqATezaiZv3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# batch_size: The dataset usually chunked into batches. batch_size=4 means after analyzing 4 batches the model updats its parameter.\n",
        "# num_workers means the number of CPU threads will be used for parallel processing.\n",
        "\n",
        "def create_dataloader_v1(txt, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\") # Initializes the BPE tokenizer.\n",
        "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride) # It creates an instance of the GPTDatasetV1.\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        drop_last=drop_last, # drop_last = True; drops the last batch if it is shorter than the specified batch_size to prevent loss spikes during training.\n",
        "        num_workers=num_workers # The number of CPU processes to use for preprocessing.\n",
        "    )\n",
        "\n",
        "    return dataloader"
      ],
      "metadata": {
        "id": "curYxxudeUFs"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test the dataloader with a batch_size=1 for an LLM with a context_size of 4 to develop an intuition of how the GPTDatasetV1 class and the create_dataloader_v1 function work together."
      ],
      "metadata": {
        "id": "O-uNpVYFnyQq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = create_dataloader_v1(raw_text, batch_size=1, max_length=4, stride=1, shuffle=False)\n",
        "data_iter = iter(dataloader)\n",
        "first_batch = next(data_iter)\n",
        "print(\"1st batch: \", first_batch) # [tensor(input ), tensor(output)]\n",
        "\n",
        "second_batch = next(data_iter)\n",
        "print(\"2nd batch: \", second_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIozy9gJoMrz",
        "outputId": "288f7764-fdf5-445e-8eb1-df48480edd4a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1st batch:  [tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n",
            "2nd batch:  [tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that small batch sizes require less memory during training but lead to more noisy model updates. Just like in regular deep learning, the batch size is a tradeoff and\n",
        "a hyperparameter to experiment with when training LLMs. Let’s look briefly at how we can use the data loader to sample with a batch size\n",
        "greater than 1:"
      ],
      "metadata": {
        "id": "oac57kmlrhG6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
        "\n",
        "# Note that we increase the stride to 4 to utilize the data set fully (we don’t skip a single word). This avoids any overlap between the batches since more overlap could lead to increased overfitting.\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)\n",
        "print(\"Inputs:\\n\", inputs)\n",
        "print(\"\\nTargets:\\n\", targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2FGD6Rerrxw",
        "outputId": "3095840a-da2e-479e-8945-b9fa9d54e045"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs:\n",
            " tensor([[   40,   367,  2885,  1464],\n",
            "        [ 1807,  3619,   402,   271],\n",
            "        [10899,  2138,   257,  7026],\n",
            "        [15632,   438,  2016,   257],\n",
            "        [  922,  5891,  1576,   438],\n",
            "        [  568,   340,   373,   645],\n",
            "        [ 1049,  5975,   284,   502],\n",
            "        [  284,  3285,   326,    11]])\n",
            "\n",
            "Targets:\n",
            " tensor([[  367,  2885,  1464,  1807],\n",
            "        [ 3619,   402,   271, 10899],\n",
            "        [ 2138,   257,  7026, 15632],\n",
            "        [  438,  2016,   257,   922],\n",
            "        [ 5891,  1576,   438,   568],\n",
            "        [  340,   373,   645,  1049],\n",
            "        [ 5975,   284,   502,   284],\n",
            "        [ 3285,   326,    11,   287]])\n"
          ]
        }
      ]
    }
  ]
}