{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPNwR1ZVDaVju6og81WV1Iy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abdussahid26/Dara-preparation-and-sampling-for-LLMs/blob/main/Tokenizing_Text_and_Converting_Tokens_into_Token_IDs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwQCg8yRYjbp",
        "outputId": "a7fe0307-c725-494e-d4e6-8f36f736d4eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.12.14)\n"
          ]
        }
      ],
      "source": [
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tokenizing Text**\n"
      ],
      "metadata": {
        "id": "LHu2ruESYr-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from importlib.metadata import version\n",
        "\n",
        "print(\"torch version:\", version(\"torch\"))\n",
        "print(\"tiktoken version:\", version(\"tiktoken\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrOG847YY08b",
        "outputId": "d2ab2d23-b815-4075-961d-8b902cdf9ce5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch version: 2.5.1+cu121\n",
            "tiktoken version: 0.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading the data file \"the-verdict.txt\"\n"
      ],
      "metadata": {
        "id": "BwAhNub9axbh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib.request\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/abdussahid26/Dara-preparation-and-sampling-for-LLMs/main/the-verdict.txt\"\n",
        "file_path = \"the-verdict.txt\"\n",
        "urllib.request.urlretrieve(url, file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Io6FU-HUZLy6",
        "outputId": "68e72b4c-af8e-4074-f288-f6dbfc16f1e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('the-verdict.txt', <http.client.HTTPMessage at 0x79140359c070>)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the \"the-verdict.txt\" file"
      ],
      "metadata": {
        "id": "Dc-kBx-ea6HE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "print(\"Total number of character: \", len(raw_text))\n",
        "print(raw_text[:99])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bpBfG6kTa__i",
        "outputId": "5a3db14f-b66c-4b23-f106-638fe351790c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of character:  20479\n",
            "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applying tokenization to the raw text"
      ],
      "metadata": {
        "id": "65Z5GvetdfjB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re # Importing regular expression (re) library\n",
        "\n",
        "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text) # \\s for whitespace\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()] # Strip whitespace from each item and then filter out any empty strings.\n",
        "print(\"Total number of token: \", len(preprocessed))\n",
        "print(preprocessed[:99])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKaGRB8_bW12",
        "outputId": "de51e11c-d539-4f77-fa1e-1012f02d8f49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of token:  4690\n",
            "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in', 'the', 'height', 'of', 'his', 'glory', ',', 'he', 'had', 'dropped', 'his', 'painting', ',', 'married', 'a', 'rich', 'widow', ',', 'and', 'established', 'himself', 'in', 'a', 'villa', 'on', 'the', 'Riviera', '.', '(', 'Though', 'I', 'rather', 'thought', 'it', 'would', 'have', 'been', 'Rome', 'or', 'Florence', '.', ')', '\"', 'The', 'height', 'of', 'his', 'glory', '\"', '--', 'that', 'was', 'what', 'the', 'women', 'called', 'it', '.', 'I', 'can', 'hear', 'Mrs', '.', 'Gideon', 'Thwing', '--', 'his', 'last', 'Chicago', 'sitter']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Converting tokens into token IDs**\n"
      ],
      "metadata": {
        "id": "6s-TO7O8e-6p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By using 'set' we create a list of all **unique** tokens and sort them alphabetically to determine the vocabulary size."
      ],
      "metadata": {
        "id": "e6LErWiejp0I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = list(set(preprocessed))\n",
        "print(\"Vocabulary size: \", len(all_words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8Vc-LYLfEHB",
        "outputId": "28147dbb-9e85-486a-9de6-2d2e8ec3684e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size:  1130\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a vocabulary"
      ],
      "metadata": {
        "id": "FPmV2HL0kdls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {token:integer for integer, token in enumerate(all_words)}\n",
        "\n",
        "for i, item in enumerate(vocab.items()):\n",
        "  print(item)\n",
        "  if i >= 99:\n",
        "    break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4i4LuSX7kggg",
        "outputId": "2294e6a0-5fb9-4b2c-84f8-ec2d1fded70a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('drew', 0)\n",
            "('weeks', 1)\n",
            "('lucky', 2)\n",
            "('clear', 3)\n",
            "('sitters', 4)\n",
            "('object', 5)\n",
            "('everlasting', 6)\n",
            "('throwing', 7)\n",
            "('wife', 8)\n",
            "('tottering', 9)\n",
            "('fostered', 10)\n",
            "('corner', 11)\n",
            "('ones', 12)\n",
            "('patient', 13)\n",
            "('companion', 14)\n",
            "('saw', 15)\n",
            "('Come', 16)\n",
            "('destroyed', 17)\n",
            "('upstairs', 18)\n",
            "('multiplied', 19)\n",
            "('wonder', 20)\n",
            "('distinguished', 21)\n",
            "('live', 22)\n",
            "('accustomed', 23)\n",
            "('?', 24)\n",
            "('interesting', 25)\n",
            "('activity', 26)\n",
            "('You', 27)\n",
            "('pink', 28)\n",
            "('Jove', 29)\n",
            "('fancy', 30)\n",
            "('brings', 31)\n",
            "('veins', 32)\n",
            "('--', 33)\n",
            "('bric-a-brac', 34)\n",
            "('chap', 35)\n",
            "('yellow', 36)\n",
            "('full', 37)\n",
            "('extracting', 38)\n",
            "('hear', 39)\n",
            "('saying', 40)\n",
            "('sensation', 41)\n",
            "('minute', 42)\n",
            "('Burlington', 43)\n",
            "('packed', 44)\n",
            "('says', 45)\n",
            "('Chicago', 46)\n",
            "('Gallery', 47)\n",
            "('long', 48)\n",
            "('though', 49)\n",
            "('his', 50)\n",
            "('genial', 51)\n",
            "('took', 52)\n",
            "('Professional', 53)\n",
            "('breathing', 54)\n",
            "('voice', 55)\n",
            "('slightly', 56)\n",
            "('yourself', 57)\n",
            "('instructive', 58)\n",
            "('after', 59)\n",
            "('kind', 60)\n",
            "('It', 61)\n",
            "('vista', 62)\n",
            "('come', 63)\n",
            "('finality', 64)\n",
            "('breaking', 65)\n",
            "('congesting', 66)\n",
            "('bear', 67)\n",
            "('laughed', 68)\n",
            "('complex', 69)\n",
            "('t', 70)\n",
            "('when', 71)\n",
            "('Made', 72)\n",
            "('touched', 73)\n",
            "('traps', 74)\n",
            "('easel', 75)\n",
            "('if', 76)\n",
            "('disease', 77)\n",
            "('terrace', 78)\n",
            "('background', 79)\n",
            "('characteristic', 80)\n",
            "('bravura', 81)\n",
            "('one', 82)\n",
            "('so', 83)\n",
            "('least', 84)\n",
            "('Among', 85)\n",
            "('slight', 86)\n",
            "('then', 87)\n",
            "('passages', 88)\n",
            "('alive', 89)\n",
            "('into', 90)\n",
            "('very', 91)\n",
            "('would', 92)\n",
            "('palm-trees', 93)\n",
            "('you', 94)\n",
            "('they', 95)\n",
            "('superb', 96)\n",
            "('purely', 97)\n",
            "('immediately', 98)\n",
            "('were', 99)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now modify the vocabulary to include two special tokens, <|unk|> and <|endoftext|>, by adding them to our list of all unique words."
      ],
      "metadata": {
        "id": "N35REOma8S7y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_tokens = sorted (list(set(preprocessed)))\n",
        "all_tokens.extend([\"<|unk|>\", \"|endoftext|\"])\n",
        "vocab = {token:integer for integer, token in enumerate(all_tokens)}\n",
        "\n",
        "for i, item in enumerate(list(vocab.items())[-5:]):\n",
        "  print(item)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xXhw3vj8ieS",
        "outputId": "87215eaa-2dcd-4862-9d0c-2bfd91cf9cea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('younger', 1127)\n",
            "('your', 1128)\n",
            "('yourself', 1129)\n",
            "('<|unk|>', 1130)\n",
            "('|endoftext|', 1131)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create a complete text tokenizer class with encoding and decoding capabilities in python\n",
        "\n"
      ],
      "metadata": {
        "id": "dYSOOGZsmlC-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizerV1:\n",
        "  def __init__(self, vocab):\n",
        "    self.str_to_int = vocab\n",
        "    self.int_to_str = {i:s for s, i in vocab.items()}\n",
        "\n",
        "  def encode(self, text):\n",
        "    preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text) # \\s for whitespace\n",
        "    preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "    ids = [self.str_to_int[s] for s in preprocessed]\n",
        "    return ids\n",
        "\n",
        "  def decode(self, ids):\n",
        "    text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "    text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text) # Removes spaces before the specified puntuation.\n",
        "    return text"
      ],
      "metadata": {
        "id": "cIGoUGl0mvEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's instantiate a new tokenizer object from the ***SimpleTokenizerV1*** class and tokenize the text from the Edith Wharton's short story."
      ],
      "metadata": {
        "id": "Yr9KRagfquGi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleTokenizerV1(vocab)\n",
        "ids = tokenizer.encode(raw_text)\n",
        "print(ids[:99])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvrxtRSEq_wg",
        "outputId": "3a1b97a0-9fa3-4ac8-ff68-2089a6298a49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[53, 44, 149, 1003, 57, 38, 818, 115, 256, 486, 6, 1002, 115, 500, 435, 392, 6, 908, 585, 1077, 709, 508, 961, 1016, 663, 1016, 535, 987, 5, 568, 988, 538, 722, 549, 496, 5, 533, 514, 370, 549, 748, 5, 661, 115, 841, 1102, 5, 157, 397, 547, 568, 115, 1066, 727, 988, 84, 7, 3, 99, 53, 818, 1003, 585, 1120, 530, 208, 85, 734, 34, 7, 4, 1, 93, 538, 722, 549, 496, 1, 6, 987, 1077, 1089, 988, 1112, 242, 585, 7, 53, 244, 535, 67, 7, 37, 100, 6, 549, 602, 25, 897]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's turn these token IDs back into text using the decode method"
      ],
      "metadata": {
        "id": "FHBhsnzot-_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(ids[:99]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXBTnOjQuGw8",
        "outputId": "e46a7a73-ea6f-4a25-ce3f-aee0ba253e60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I HAD always thought Jack Gisburn rather a cheap genius -- though a good fellow enough -- so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a rich widow, and established himself in a villa on the Riviera.( Though I rather thought it would have been Rome or Florence.)\" The height of his glory\" -- that was what the women called it. I can hear Mrs. Gideon Thwing -- his last Chicago sitter\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Adding special context tokens**\n"
      ],
      "metadata": {
        "id": "7j6MY_u3v_uG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we'll modify the tokenizer to handle unknown words."
      ],
      "metadata": {
        "id": "N5gzKFOTwPOt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizerV2:\n",
        "  def __init__(self, vocab):\n",
        "    self.str_to_int = vocab\n",
        "    self.int_to_str = { i:s for s,i in vocab.items()}\n",
        "  def encode(self, text):\n",
        "    preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "    preprocessed = [\n",
        "    item.strip() for item in preprocessed if item.strip()\n",
        "    ]\n",
        "    preprocessed = [item if item in self.str_to_int\n",
        "    else \"<|unk|>\" for item in preprocessed]\n",
        "    ids = [self.str_to_int[s] for s in preprocessed]\n",
        "    return ids\n",
        "  def decode(self, ids):\n",
        "#    text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "    text = \" \".join([self.int_to_str[i] if i in self.int_to_str else \"|unk|\" for i in ids])\n",
        "    text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "88InFybZwG8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleTokenizerV2(vocab)\n",
        "ids = tokenizer.encode(raw_text)\n",
        "print(ids[:99])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5PWAHx518SX",
        "outputId": "ea9eb319-8e2a-48ff-e089-de95cd55bb87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[53, 44, 149, 1003, 57, 38, 818, 115, 256, 486, 6, 1002, 115, 500, 435, 392, 6, 908, 585, 1077, 709, 508, 961, 1016, 663, 1016, 535, 987, 5, 568, 988, 538, 722, 549, 496, 5, 533, 514, 370, 549, 748, 5, 661, 115, 841, 1102, 5, 157, 397, 547, 568, 115, 1066, 727, 988, 84, 7, 3, 99, 53, 818, 1003, 585, 1120, 530, 208, 85, 734, 34, 7, 4, 1, 93, 538, 722, 549, 496, 1, 6, 987, 1077, 1089, 988, 1112, 242, 585, 7, 53, 244, 535, 67, 7, 37, 100, 6, 549, 602, 25, 897]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode([1122]))\n",
        "print(tokenizer.decode([9113]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9umycebm18ar",
        "outputId": "5f4585a8-a18c-4ca4-dd29-2f539d18a3d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "year\n",
            "|unk|\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text1 = \"Hello, do you like tea?\"\n",
        "text2 = \"In the sunlit terraces of the palace.\"\n",
        "combined_text = \" <|endoftext|> \".join((text1, text2))\n",
        "print(combined_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aA5Cf7Vk4WOv",
        "outputId": "f00011b6-5da0-4d50-84c3-fc1fb1c10600"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleTokenizerV2(vocab)\n",
        "print(tokenizer.encode(combined_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h839WNpX7ZUZ",
        "outputId": "83d82f55-4ba7-48fb-af1b-3abb637f308c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1130, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1130, 7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(tokenizer.encode(combined_text)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VcgJUvBq_zn1",
        "outputId": "c86328e1-edc0-4ccb-af8c-89ae0e240bd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|unk|>, do you like tea? <|unk|> In the sunlit terraces of the <|unk|>.\n"
          ]
        }
      ]
    }
  ]
}