{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMIbcJa7kIZ0PG636Cc2ol4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abdussahid26/Dara-preparation-and-sampling-for-LLMs/blob/main/Tokenizing_Text_and_Converting_Tokens_into_Token_IDs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwQCg8yRYjbp",
        "outputId": "50b6a21f-430d-49a7-afb6-1a335bfab82a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.12.14)\n"
          ]
        }
      ],
      "source": [
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tokenizing Text**\n"
      ],
      "metadata": {
        "id": "LHu2ruESYr-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from importlib.metadata import version\n",
        "\n",
        "print(\"torch version:\", version(\"torch\"))\n",
        "print(\"tiktoken version:\", version(\"tiktoken\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrOG847YY08b",
        "outputId": "eb0e3bd3-c255-4edd-96b8-0d2c0c0aaf09"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch version: 2.5.1+cu121\n",
            "tiktoken version: 0.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading the data file \"the-verdict.txt\"\n"
      ],
      "metadata": {
        "id": "BwAhNub9axbh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib.request\n",
        "\n",
        "url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
        "       \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
        "       \"the-verdict.txt\")\n",
        "file_path = \"the-verdict.txt\"\n",
        "urllib.request.urlretrieve(url, file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Io6FU-HUZLy6",
        "outputId": "dff94eb2-12f9-4b3b-81ba-7b27b19485e9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('the-verdict.txt', <http.client.HTTPMessage at 0x789406192e30>)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the \"the-verdict.txt\" file"
      ],
      "metadata": {
        "id": "Dc-kBx-ea6HE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "print(\"Total number of character: \", len(raw_text))\n",
        "print(raw_text[:99])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bpBfG6kTa__i",
        "outputId": "b85af4b3-d6ff-4b0d-b1b2-9bb897ea6d9c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of character:  20479\n",
            "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applying tokenization to the raw text"
      ],
      "metadata": {
        "id": "65Z5GvetdfjB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re # Importing regular expression (re) library\n",
        "\n",
        "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text) # \\s for whitespace\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()] # Strip whitespace from each item and then filter out any empty strings.\n",
        "print(\"Total number of token: \", len(preprocessed))\n",
        "print(preprocessed[:99])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKaGRB8_bW12",
        "outputId": "f9b62563-a4c5-47c8-fda6-79eeed2e494c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of token:  4690\n",
            "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in', 'the', 'height', 'of', 'his', 'glory', ',', 'he', 'had', 'dropped', 'his', 'painting', ',', 'married', 'a', 'rich', 'widow', ',', 'and', 'established', 'himself', 'in', 'a', 'villa', 'on', 'the', 'Riviera', '.', '(', 'Though', 'I', 'rather', 'thought', 'it', 'would', 'have', 'been', 'Rome', 'or', 'Florence', '.', ')', '\"', 'The', 'height', 'of', 'his', 'glory', '\"', '--', 'that', 'was', 'what', 'the', 'women', 'called', 'it', '.', 'I', 'can', 'hear', 'Mrs', '.', 'Gideon', 'Thwing', '--', 'his', 'last', 'Chicago', 'sitter']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Converting tokens into token IDs**\n"
      ],
      "metadata": {
        "id": "6s-TO7O8e-6p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By using 'set' we create a list of all **unique** tokens and sort them alphabetically to determine the vocabulary size."
      ],
      "metadata": {
        "id": "e6LErWiejp0I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = list(set(preprocessed))\n",
        "print(\"Vocabulary size: \", len(all_words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8Vc-LYLfEHB",
        "outputId": "47ac2194-f79b-4912-e335-cce8d844dcb6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size:  1130\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a vocabulary"
      ],
      "metadata": {
        "id": "FPmV2HL0kdls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {token:integer for integer, token in enumerate(all_words)}\n",
        "\n",
        "for i, item in enumerate(vocab.items()):\n",
        "  print(item)\n",
        "  if i >= 99:\n",
        "    break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4i4LuSX7kggg",
        "outputId": "a57f4d0e-868f-4eb0-990c-ea97ae0004ef"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('bric-a-brac', 0)\n",
            "('doesn', 1)\n",
            "('sitters', 2)\n",
            "('arms', 3)\n",
            "('lit', 4)\n",
            "('Florence', 5)\n",
            "('unaccountable', 6)\n",
            "('perfect', 7)\n",
            "('nervous', 8)\n",
            "('false', 9)\n",
            "('cigars', 10)\n",
            "('line', 11)\n",
            "('been', 12)\n",
            "('thought', 13)\n",
            "('Never', 14)\n",
            "('Hermia', 15)\n",
            "('arm-chair', 16)\n",
            "('unusual', 17)\n",
            "('instructive', 18)\n",
            "('tribute', 19)\n",
            "('mood', 20)\n",
            "('art', 21)\n",
            "('stand', 22)\n",
            "('domestic', 23)\n",
            "('stammer', 24)\n",
            "('crossed', 25)\n",
            "('know', 26)\n",
            "('curtains', 27)\n",
            "('Professional', 28)\n",
            "('close', 29)\n",
            "('nymphs', 30)\n",
            "('event', 31)\n",
            "('eyebrows', 32)\n",
            "('Grindles', 33)\n",
            "('widow', 34)\n",
            "('caught', 35)\n",
            "('eighteenth-century', 36)\n",
            "('slightly', 37)\n",
            "('faces', 38)\n",
            "('run', 39)\n",
            "('bath-rooms', 40)\n",
            "('ah', 41)\n",
            "('proclaiming', 42)\n",
            "('says', 43)\n",
            "('anywhere', 44)\n",
            "('poor', 45)\n",
            "('who', 46)\n",
            "('count', 47)\n",
            "('things', 48)\n",
            "('damask', 49)\n",
            "('ironic', 50)\n",
            "('find', 51)\n",
            "('stocked', 52)\n",
            "('across', 53)\n",
            "('simpleton', 54)\n",
            "('then', 55)\n",
            "('balustraded', 56)\n",
            "('mere', 57)\n",
            "('moved', 58)\n",
            "('garlands', 59)\n",
            "('Come', 60)\n",
            "('The', 61)\n",
            "('lies', 62)\n",
            "('shirked', 63)\n",
            "('don', 64)\n",
            "('eyes', 65)\n",
            "('stream', 66)\n",
            "('detail', 67)\n",
            "('forehead', 68)\n",
            "('dead', 69)\n",
            "('added', 70)\n",
            "('Begin', 71)\n",
            "('just', 72)\n",
            "('colour', 73)\n",
            "('forgive', 74)\n",
            "('but', 75)\n",
            "('she', 76)\n",
            "('packed', 77)\n",
            "('wander', 78)\n",
            "('What', 79)\n",
            "('every', 80)\n",
            "('life', 81)\n",
            "('sunburn', 82)\n",
            "('admirers', 83)\n",
            "('was', 84)\n",
            "('anything', 85)\n",
            "('fellow', 86)\n",
            "('couldn', 87)\n",
            "('chair', 88)\n",
            "('half-light', 89)\n",
            "('thing', 90)\n",
            "('sketch', 91)\n",
            "('could', 92)\n",
            "('terra-cotta', 93)\n",
            "('His', 94)\n",
            "('year', 95)\n",
            "('some', 96)\n",
            "('awful', 97)\n",
            "('faith', 98)\n",
            "('morbidly', 99)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now modify the vocabulary to include two special tokens, <|unk|> and <|endoftext|>, by adding them to our list of all unique words."
      ],
      "metadata": {
        "id": "N35REOma8S7y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_tokens = sorted (list(set(preprocessed)))\n",
        "all_tokens.extend([\"<|unk|>\", \"|endoftext|\"])\n",
        "vocab = {token:integer for integer, token in enumerate(all_tokens)}\n",
        "\n",
        "for i, item in enumerate(list(vocab.items())[-5:]):\n",
        "  print(item)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xXhw3vj8ieS",
        "outputId": "b4f6452c-ee3b-481b-8e5f-b1345fddcbdd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('younger', 1127)\n",
            "('your', 1128)\n",
            "('yourself', 1129)\n",
            "('<|unk|>', 1130)\n",
            "('|endoftext|', 1131)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create a complete text tokenizer class with encoding and decoding capabilities in python\n",
        "\n"
      ],
      "metadata": {
        "id": "dYSOOGZsmlC-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizerV1:\n",
        "  def __init__(self, vocab):\n",
        "    self.str_to_int = vocab\n",
        "    self.int_to_str = {i:s for s, i in vocab.items()}\n",
        "\n",
        "  def encode(self, text):\n",
        "    preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text) # \\s for whitespace\n",
        "    preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "    ids = [self.str_to_int[s] for s in preprocessed]\n",
        "    return ids\n",
        "\n",
        "  def decode(self, ids):\n",
        "    text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "    text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text) # Removes spaces before the specified puntuation.\n",
        "    return text"
      ],
      "metadata": {
        "id": "cIGoUGl0mvEl"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's instantiate a new tokenizer object from the ***SimpleTokenizerV1*** class and tokenize the text from the Edith Wharton's short story."
      ],
      "metadata": {
        "id": "Yr9KRagfquGi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleTokenizerV1(vocab)\n",
        "ids = tokenizer.encode(raw_text)\n",
        "print(ids[:99])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvrxtRSEq_wg",
        "outputId": "a73b1ae0-ad90-43a4-9711-e78ba27ddd98"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[53, 44, 149, 1003, 57, 38, 818, 115, 256, 486, 6, 1002, 115, 500, 435, 392, 6, 908, 585, 1077, 709, 508, 961, 1016, 663, 1016, 535, 987, 5, 568, 988, 538, 722, 549, 496, 5, 533, 514, 370, 549, 748, 5, 661, 115, 841, 1102, 5, 157, 397, 547, 568, 115, 1066, 727, 988, 84, 7, 3, 99, 53, 818, 1003, 585, 1120, 530, 208, 85, 734, 34, 7, 4, 1, 93, 538, 722, 549, 496, 1, 6, 987, 1077, 1089, 988, 1112, 242, 585, 7, 53, 244, 535, 67, 7, 37, 100, 6, 549, 602, 25, 897]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's turn these token IDs back into text using the decode method"
      ],
      "metadata": {
        "id": "FHBhsnzot-_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(ids[:99]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXBTnOjQuGw8",
        "outputId": "f71c4ad9-2098-4342-9bbe-a5bf96231e2c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I HAD always thought Jack Gisburn rather a cheap genius -- though a good fellow enough -- so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a rich widow, and established himself in a villa on the Riviera.( Though I rather thought it would have been Rome or Florence.)\" The height of his glory\" -- that was what the women called it. I can hear Mrs. Gideon Thwing -- his last Chicago sitter\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Adding special context tokens**\n"
      ],
      "metadata": {
        "id": "7j6MY_u3v_uG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we'll modify the tokenizer to handle unknown words."
      ],
      "metadata": {
        "id": "N5gzKFOTwPOt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizerV2:\n",
        "  def __init__(self, vocab):\n",
        "    self.str_to_int = vocab\n",
        "    self.int_to_str = { i:s for s,i in vocab.items()}\n",
        "  def encode(self, text):\n",
        "    preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "    preprocessed = [\n",
        "    item.strip() for item in preprocessed if item.strip()\n",
        "    ]\n",
        "    preprocessed = [item if item in self.str_to_int\n",
        "    else \"<|unk|>\" for item in preprocessed]\n",
        "    ids = [self.str_to_int[s] for s in preprocessed]\n",
        "    return ids\n",
        "  def decode(self, ids):\n",
        "#    text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "    text = \" \".join([self.int_to_str[i] if i in self.int_to_str else \"|unk|\" for i in ids])\n",
        "    text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "88InFybZwG8K"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleTokenizerV2(vocab)\n",
        "ids = tokenizer.encode(raw_text)\n",
        "print(ids[:99])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5PWAHx518SX",
        "outputId": "ffb27ec6-145e-4270-9fc8-c31d6d25d597"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[53, 44, 149, 1003, 57, 38, 818, 115, 256, 486, 6, 1002, 115, 500, 435, 392, 6, 908, 585, 1077, 709, 508, 961, 1016, 663, 1016, 535, 987, 5, 568, 988, 538, 722, 549, 496, 5, 533, 514, 370, 549, 748, 5, 661, 115, 841, 1102, 5, 157, 397, 547, 568, 115, 1066, 727, 988, 84, 7, 3, 99, 53, 818, 1003, 585, 1120, 530, 208, 85, 734, 34, 7, 4, 1, 93, 538, 722, 549, 496, 1, 6, 987, 1077, 1089, 988, 1112, 242, 585, 7, 53, 244, 535, 67, 7, 37, 100, 6, 549, 602, 25, 897]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode([1122]))\n",
        "print(tokenizer.decode([9113]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9umycebm18ar",
        "outputId": "456309c9-7649-4fd0-f4ef-521c2940864f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "year\n",
            "|unk|\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text1 = \"Hello, do you like tea?\"\n",
        "text2 = \"In the sunlit terraces of the palace.\"\n",
        "combined_text = \" <|endoftext|> \".join((text1, text2))\n",
        "print(combined_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aA5Cf7Vk4WOv",
        "outputId": "1291da67-1c21-4063-9bf8-d81f90e5feaf"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleTokenizerV2(vocab)\n",
        "print(tokenizer.encode(combined_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h839WNpX7ZUZ",
        "outputId": "38b04748-0bac-4ee3-8f06-6aaa39457ed1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1130, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1130, 7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(tokenizer.encode(combined_text)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VcgJUvBq_zn1",
        "outputId": "10654de6-c3c8-4b18-fb29-48cc99ab3dcd"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|unk|>, do you like tea? <|unk|> In the sunlit terraces of the <|unk|>.\n"
          ]
        }
      ]
    }
  ]
}