{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNewDmdlQgI0ST0NNEJpZmR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abdussahid26/Dara-preparation-and-sampling-for-LLMs/blob/main/Token_Embedding_and_Positional_Embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Small hands on demo: Playing with token embeddings**"
      ],
      "metadata": {
        "id": "2SvEZ-ldFRTC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import trained model"
      ],
      "metadata": {
        "id": "S-ZH1XYI986Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cF0t5U5t95RT"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "model = api.load(\"word2vec-google-news-300\") # download the model and return as object ready for use"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_vectors = model\n",
        "\n",
        "# Let us look how the vector embedding of a word looks like\n",
        "print(word_vectors['computer']) # Accessing the vector or token embedding for the word 'computer'."
      ],
      "metadata": {
        "id": "Ittq3LtJ-cNu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "014a2f31-f4e2-42c1-f1cd-1cdeaa8254ff"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 1.07421875e-01 -2.01171875e-01  1.23046875e-01  2.11914062e-01\n",
            " -9.13085938e-02  2.16796875e-01 -1.31835938e-01  8.30078125e-02\n",
            "  2.02148438e-01  4.78515625e-02  3.66210938e-02 -2.45361328e-02\n",
            "  2.39257812e-02 -1.60156250e-01 -2.61230469e-02  9.71679688e-02\n",
            " -6.34765625e-02  1.84570312e-01  1.70898438e-01 -1.63085938e-01\n",
            " -1.09375000e-01  1.49414062e-01 -4.65393066e-04  9.61914062e-02\n",
            "  1.68945312e-01  2.60925293e-03  8.93554688e-02  6.49414062e-02\n",
            "  3.56445312e-02 -6.93359375e-02 -1.46484375e-01 -1.21093750e-01\n",
            " -2.27539062e-01  2.45361328e-02 -1.24511719e-01 -3.18359375e-01\n",
            " -2.20703125e-01  1.30859375e-01  3.66210938e-02 -3.63769531e-02\n",
            " -1.13281250e-01  1.95312500e-01  9.76562500e-02  1.26953125e-01\n",
            "  6.59179688e-02  6.93359375e-02  1.02539062e-02  1.75781250e-01\n",
            " -1.68945312e-01  1.21307373e-03 -2.98828125e-01 -1.15234375e-01\n",
            "  5.66406250e-02 -1.77734375e-01 -2.08984375e-01  1.76757812e-01\n",
            "  2.38037109e-02 -2.57812500e-01 -4.46777344e-02  1.88476562e-01\n",
            "  5.51757812e-02  5.02929688e-02 -1.06933594e-01  1.89453125e-01\n",
            " -1.16210938e-01  8.49609375e-02 -1.71875000e-01  2.45117188e-01\n",
            " -1.73828125e-01 -8.30078125e-03  4.56542969e-02 -1.61132812e-02\n",
            "  1.86523438e-01 -6.05468750e-02 -4.17480469e-02  1.82617188e-01\n",
            "  2.20703125e-01 -1.22558594e-01 -2.55126953e-02 -3.08593750e-01\n",
            "  9.13085938e-02  1.60156250e-01  1.70898438e-01  1.19628906e-01\n",
            "  7.08007812e-02 -2.64892578e-02 -3.08837891e-02  4.06250000e-01\n",
            " -1.01562500e-01  5.71289062e-02 -7.26318359e-03 -9.17968750e-02\n",
            " -1.50390625e-01 -2.55859375e-01  2.16796875e-01 -3.63769531e-02\n",
            "  2.24609375e-01  8.00781250e-02  1.56250000e-01  5.27343750e-02\n",
            "  1.50390625e-01 -1.14746094e-01 -8.64257812e-02  1.19140625e-01\n",
            " -7.17773438e-02  2.73437500e-01 -1.64062500e-01  7.29370117e-03\n",
            "  4.21875000e-01 -1.12792969e-01 -1.35742188e-01 -1.31835938e-01\n",
            " -1.37695312e-01 -7.66601562e-02  6.25000000e-02  4.98046875e-02\n",
            " -1.91406250e-01 -6.03027344e-02  2.27539062e-01  5.88378906e-02\n",
            " -3.24218750e-01  5.41992188e-02 -1.35742188e-01  8.17871094e-03\n",
            " -5.24902344e-02 -1.74713135e-03 -9.81445312e-02 -2.86865234e-02\n",
            "  3.61328125e-02  2.15820312e-01  5.98144531e-02 -3.08593750e-01\n",
            " -2.27539062e-01  2.61718750e-01  9.86328125e-02 -5.07812500e-02\n",
            "  1.78222656e-02  1.31835938e-01 -5.35156250e-01 -1.81640625e-01\n",
            "  1.38671875e-01 -3.10546875e-01 -9.71679688e-02  1.31835938e-01\n",
            " -1.16210938e-01  7.03125000e-02  2.85156250e-01  3.51562500e-02\n",
            " -1.01562500e-01 -3.75976562e-02  1.41601562e-01  1.42578125e-01\n",
            " -5.68847656e-02  2.65625000e-01 -2.09960938e-01  9.64355469e-03\n",
            " -6.68945312e-02 -4.83398438e-02 -6.10351562e-02  2.45117188e-01\n",
            " -9.66796875e-02  1.78222656e-02 -1.27929688e-01 -4.78515625e-02\n",
            " -7.26318359e-03  1.79687500e-01  2.78320312e-02 -2.10937500e-01\n",
            " -1.43554688e-01 -1.27929688e-01  1.73339844e-02 -3.60107422e-03\n",
            " -2.04101562e-01  3.63159180e-03 -1.19628906e-01 -6.15234375e-02\n",
            "  5.93261719e-02 -3.23486328e-03 -1.70898438e-01 -3.14941406e-02\n",
            " -8.88671875e-02 -2.89062500e-01  3.44238281e-02 -1.87500000e-01\n",
            "  2.94921875e-01  1.58203125e-01 -1.19628906e-01  7.61718750e-02\n",
            "  6.39648438e-02 -4.68750000e-02 -6.83593750e-02  1.21459961e-02\n",
            " -1.44531250e-01  4.54101562e-02  3.68652344e-02  3.88671875e-01\n",
            "  1.45507812e-01 -2.55859375e-01 -4.46777344e-02 -1.33789062e-01\n",
            " -1.38671875e-01  6.59179688e-02  1.37695312e-01  1.14746094e-01\n",
            "  2.03125000e-01 -4.78515625e-02  1.80664062e-02 -8.54492188e-02\n",
            " -2.48046875e-01 -3.39843750e-01 -2.83203125e-02  1.05468750e-01\n",
            " -2.14843750e-01 -8.74023438e-02  7.12890625e-02  1.87500000e-01\n",
            " -1.12304688e-01  2.73437500e-01 -3.26171875e-01 -1.77734375e-01\n",
            " -4.24804688e-02 -2.69531250e-01  6.64062500e-02 -6.88476562e-02\n",
            " -1.99218750e-01 -7.03125000e-02 -2.43164062e-01 -3.66210938e-02\n",
            " -7.37304688e-02 -1.77734375e-01  9.17968750e-02 -1.25000000e-01\n",
            " -1.65039062e-01 -3.57421875e-01 -2.85156250e-01 -1.66992188e-01\n",
            "  1.97265625e-01 -1.53320312e-01  2.31933594e-02  2.06054688e-01\n",
            "  1.80664062e-01 -2.74658203e-02 -1.92382812e-01 -9.61914062e-02\n",
            " -1.06811523e-02 -4.73632812e-02  6.54296875e-02 -1.25732422e-02\n",
            "  1.78222656e-02 -8.00781250e-02 -2.59765625e-01  9.37500000e-02\n",
            " -7.81250000e-02  4.68750000e-02 -2.22167969e-02  1.86767578e-02\n",
            "  3.11279297e-02  1.04980469e-02 -1.69921875e-01  2.58789062e-02\n",
            " -3.41796875e-02 -1.44042969e-02 -5.46875000e-02 -8.78906250e-02\n",
            "  1.96838379e-03  2.23632812e-01 -1.36718750e-01  1.75781250e-01\n",
            " -1.63085938e-01  1.87500000e-01  3.44238281e-02 -5.63964844e-02\n",
            " -2.27689743e-05  4.27246094e-02  5.81054688e-02 -1.07910156e-01\n",
            " -3.88183594e-02 -2.69531250e-01  3.34472656e-02  9.81445312e-02\n",
            "  5.63964844e-02  2.23632812e-01 -5.49316406e-02  1.46484375e-01\n",
            "  5.93261719e-02 -2.19726562e-01  6.39648438e-02  1.66015625e-02\n",
            "  4.56542969e-02  3.26171875e-01 -3.80859375e-01  1.70898438e-01\n",
            "  5.66406250e-02 -1.04492188e-01  1.38671875e-01 -1.57226562e-01\n",
            "  3.23486328e-03 -4.80957031e-02 -2.48046875e-01 -6.20117188e-02]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example of a word as a vector**"
      ],
      "metadata": {
        "id": "dc_TUUiF-XZN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check the shape of a word."
      ],
      "metadata": {
        "id": "BrapBK_H_BQV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_vectors['computer'].shape)"
      ],
      "metadata": {
        "id": "R_sIkHhE_FfW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28517a4a-4611-4ae2-908d-904607081013"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(300,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Similar words\n",
        "\n",
        "King + Woman - Man = ?"
      ],
      "metadata": {
        "id": "9RT2m_lC_KoL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of using most_similar\n",
        "\n",
        "print(word_vectors.most_similar(positive=['king', 'woman'], negative=['man'], topn=10))\n",
        "\n",
        "# most_similar calculates word similarity using vector arithmetic: 'king' + 'woman' − 'man' to find the most relevant words in the vector space.\n",
        "# the parameter topn=10 specifies the number of most similar words to return based on their similarity scores."
      ],
      "metadata": {
        "id": "JYy7L_T7_ZLJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a235e29-2f59-49f0-ecea-91d31a795f3f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('queen', 0.7118193507194519), ('monarch', 0.6189674139022827), ('princess', 0.5902431011199951), ('crown_prince', 0.5499460697174072), ('prince', 0.5377321839332581), ('kings', 0.5236844420433044), ('Queen_Consort', 0.5235945582389832), ('queens', 0.5181134343147278), ('sultan', 0.5098593831062317), ('monarchy', 0.5087411999702454)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's us check the similarity between a few pairs of words"
      ],
      "metadata": {
        "id": "sxxPpF-hAKqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of calculating similarity\n",
        "\n",
        "print(word_vectors.similarity('woman', 'man'))\n",
        "print(word_vectors.similarity('king', 'queen'))\n",
        "print(word_vectors.similarity('uncle', 'aunt'))\n",
        "print(word_vectors.similarity('boy', 'girl'))\n",
        "print(word_vectors.similarity('computer', 'water'))\n",
        "print(word_vectors.similarity('paper', 'knife'))"
      ],
      "metadata": {
        "id": "0uPSs5K2Aj0Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96a1b4ed-c4f7-4302-e7ee-a46b575545b3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.76640123\n",
            "0.6510957\n",
            "0.7643474\n",
            "0.8543272\n",
            "0.07963113\n",
            "0.13640551\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most similar words"
      ],
      "metadata": {
        "id": "2ipQsyzcCVaQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_vectors.most_similar(\"computer\", topn=5))"
      ],
      "metadata": {
        "id": "Gr3cNcUZCXxL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a3977bb-754e-4492-e478-5ed943adb2e3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('computers', 0.7979379892349243), ('laptop', 0.6640493273735046), ('laptop_computer', 0.6548868417739868), ('Computer', 0.647333562374115), ('com_puter', 0.6082080006599426)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see the vector similarity"
      ],
      "metadata": {
        "id": "kMnL7fUvCn4C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Words to compare\n",
        "word1 = 'man'\n",
        "word2 = 'woman'\n",
        "\n",
        "word3 = 'semiconductor'\n",
        "word4 = 'earthworm'\n",
        "\n",
        "word5 = 'nephew'\n",
        "word6 = 'niece'\n",
        "\n",
        "# Calculate the vector difference\n",
        "vector_diff1 = model[word1] - model[word2]\n",
        "vector_diff2 = model[word3] - model[word4]\n",
        "vector_diff3 = model[word5] - model[word6]\n",
        "\n",
        "# Calculate the magnitude of the vector difference\n",
        "magnitude_of_diff1 = np.linalg.norm(vector_diff1)\n",
        "magnitude_of_diff2 = np.linalg.norm(vector_diff2)\n",
        "magnitude_of_diff3 = np.linalg.norm(vector_diff3)\n",
        "\n",
        "# Print the magnitude of the difference\n",
        "print(\"The magnitude of the difference between '{}' and '{}' is {:.2f}\".format(word1, word2, magnitude_of_diff1))\n",
        "print(\"The magnitude of the difference between '{}' and '{}' is {:.2f}\".format(word3, word4, magnitude_of_diff2))\n",
        "print(\"The magnitude of the difference between '{}' and '{}' is {:.2f}\".format(word5, word6, magnitude_of_diff3))"
      ],
      "metadata": {
        "id": "qppWjI5gCukT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec581fc4-ed6e-4526-c922-4b2168b5a4a0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The magnitude of the difference between 'man' and 'woman' is 1.73\n",
            "The magnitude of the difference between 'semiconductor' and 'earthworm' is 5.67\n",
            "The magnitude of the difference between 'nephew' and 'niece' is 1.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Token embeddings created for LLMs**"
      ],
      "metadata": {
        "id": "0Zig_DlOFlpE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how the token ID to embedding vector conversion works with a hands-on example. Suppose we have the following four input tokens with IDs 2, 3, 5, and 1:"
      ],
      "metadata": {
        "id": "VbtjXJMGJlcH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "input_ids = torch.tensor([2, 3, 5, 1])"
      ],
      "metadata": {
        "id": "7MJbAls5Ft9O"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the sake of simplicity, suppose we have a small vocabulary of only 6 words (instead\n",
        "of the 50,257 words in the BPE tokenizer vocabulary), and we want to create embeddings\n",
        "of size 3 (in GPT-3, the embedding size is 12,288 dimensions):\n"
      ],
      "metadata": {
        "id": "I6_7jchpLauP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 6\n",
        "output_dim = 3"
      ],
      "metadata": {
        "id": "BTgZ9CbqLcBN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the vocab_size and output_dim, we can instantiate an embedding layer in PyTorch, setting the random seed to 123 for reproducibility purposes."
      ],
      "metadata": {
        "id": "2Y9l60w5Lxuu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
        "print(embedding_layer.weight)\n",
        "\n",
        "# The weight matrix of the embedding layer contains small, random values. These values are optimized during LLM training as part of the LLM optimization itself.\n",
        "# Moreover, we can see that the weight matrix has six rows and three columns. There is one row for each of the six possible tokens in the vocabulary, and there is one column for each of the three embedding dimensions."
      ],
      "metadata": {
        "id": "ZZZYR-S3L9qt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38bfb37a-3887-4f79-afbb-aa06e200534d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.3374, -0.1778, -0.1690],\n",
            "        [ 0.9178,  1.5810,  1.3010],\n",
            "        [ 1.2753, -0.2010, -0.1606],\n",
            "        [-0.4015,  0.9666, -1.1481],\n",
            "        [-1.1589,  0.3255, -0.6315],\n",
            "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After we instantiated the embedding layer, let's apply it to a token ID to obtain the embedding vector."
      ],
      "metadata": {
        "id": "-OBUeL6yOIWO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_layer(torch.tensor([3])))"
      ],
      "metadata": {
        "id": "MCKIBzMgOSMk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fe58e71-05ab-4f12-f540-470a046b0a98"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we compare the embedding vector for token ID 3 to the previous embedding\n",
        "matrix, we see that it is identical to the fourth row (Python starts with a zero index, so\n",
        "it’s the row corresponding to index 3). In other words, the embedding layer is essentially\n",
        "a lookup operation that retrieves rows from the embedding layer’s weight matrix\n",
        "via a token ID."
      ],
      "metadata": {
        "id": "rmsJH7mROppr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We've seen how to convert a single token ID into a three-dimensional embedding vector. Let's now apply that to all four inputs IDs (torch.tensor([2, 3, 5, 1]))"
      ],
      "metadata": {
        "id": "3f_8LCEeOrwV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_layer(input_ids)) # Having now created embedding vectors from token IDs. Following created embedding layer is the same as Neural Network linear layer."
      ],
      "metadata": {
        "id": "EhkWrmTRO9nM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "971e9dab-2935-4992-edaf-ad7a02901d2f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.2753, -0.2010, -0.1606],\n",
            "        [-0.4015,  0.9666, -1.1481],\n",
            "        [-2.8400, -0.7849, -1.4096],\n",
            "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Positional Embeddings (Encoding Word Positions)**\n",
        "\n",
        "Previously, we focused on very small embedding sizes for simplicity. Now, let’s consider\n",
        "more realistic and useful embedding sizes and encode the input tokens into a\n",
        "256-dimensional vector representation, which is smaller than what the original GPT-3\n",
        "model used (in GPT-3, the embedding size is 12,288 dimensions) but still reasonable\n",
        "for experimentation. Furthermore, we assume that the token IDs were created by the\n",
        "BPE tokenizer we implemented earlier, which has a vocabulary size of 50,257."
      ],
      "metadata": {
        "id": "u0A5F5CrzM7B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 50257\n",
        "output_dim = 256\n",
        "\n",
        "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
      ],
      "metadata": {
        "id": "ykfeBfOGzsPP"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the previous token_embedding_layer, if we sample data from the dataloader, we embed each token in each batch into a 256-dimensional vector. If we have a batch size of 8 with 4 tokens each, the result will be an 8 x 4 x 256 tensor."
      ],
      "metadata": {
        "id": "cAJL2WOe0GBl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class GPTDatasetV1(Dataset):\n",
        "    def __init__(self, txt, tokenizer, max_length, stride): # max_length means context_size;  the stride determines how much we slide during applying sliding window approach\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        token_ids = tokenizer.encode(txt) # Tokenizes the entire text\n",
        "\n",
        "        for i in range(0, len(token_ids) - max_length, stride): # Uses a sliding window approach to chunk the book into overlapping sequences of max_length\n",
        "            input_chunk = token_ids[i:i + max_length]\n",
        "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "    def __len__(self): # Returns the total number of rows in the dataset\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx): # Returns a single row from the dataset\n",
        "        return self.input_ids[idx], self.target_ids[idx]"
      ],
      "metadata": {
        "id": "1KpzA81w176J"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# batch_size: The dataset usually chunked into batches. batch_size=4 means after analyzing 4 batches the model updats its parameter.\n",
        "# num_workers means the number of CPU threads will be used for parallel processing.\n",
        "\n",
        "def create_dataloader_v1(txt, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\") # Initializes the BPE tokenizer.\n",
        "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride) # It creates an instance of the GPTDatasetV1.\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        drop_last=drop_last, # drop_last = True; drops the last batch if it is shorter than the specified batch_size to prevent loss spikes during training.\n",
        "        num_workers=num_workers # The number of CPU processes to use for preprocessing.\n",
        "    )\n",
        "\n",
        "    return dataloader\n"
      ],
      "metadata": {
        "id": "D-gZ7Q9Z1-Si"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "id": "OYrUB36c3P8L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c23eaf7-802c-4b17-840f-713571e2bf76"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.12.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken"
      ],
      "metadata": {
        "id": "XE_cfcUF3Swu"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib.request\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/abdussahid26/Dara-preparation-and-sampling-for-LLMs/main/the-verdict.txt\"\n",
        "\n",
        "file_path = \"the-verdict.txt\"\n",
        "urllib.request.urlretrieve(url, file_path)\n",
        "\n",
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()"
      ],
      "metadata": {
        "id": "H2tuZXdU3AlN"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 4\n",
        "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=max_length, stride=max_length, shuffle=False)\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)\n",
        "print(\"Token IDs:\\n\", inputs)\n",
        "print(\"\\nInputs shape: \\n\", inputs.shape)"
      ],
      "metadata": {
        "id": "QR4RypAG0jAl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "436fc660-1ef7-4ea3-a060-e2a524f3b954"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token IDs:\n",
            " tensor([[   40,   367,  2885,  1464],\n",
            "        [ 1807,  3619,   402,   271],\n",
            "        [10899,  2138,   257,  7026],\n",
            "        [15632,   438,  2016,   257],\n",
            "        [  922,  5891,  1576,   438],\n",
            "        [  568,   340,   373,   645],\n",
            "        [ 1049,  5975,   284,   502],\n",
            "        [  284,  3285,   326,    11]])\n",
            "\n",
            "Inputs shape: \n",
            " torch.Size([8, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, the token ID tensor is 8 × 4 dimensional, meaning that the data batch consists of eight text samples with four tokens each.\n",
        "Let’s now use the embedding layer to embed these token IDs into 256-dimensional\n",
        "vectors."
      ],
      "metadata": {
        "id": "9-YuKK7R4pbW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " token_embeddings = token_embedding_layer(inputs)\n",
        " print(token_embeddings.shape)"
      ],
      "metadata": {
        "id": "C0vB2omr3gS0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d218fd29-d03b-4c1f-a4c2-52565ef34ff4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The 8 × 4 × 256–dimensional tensor output shows that each token ID is now embedded as a 256-dimensional vector."
      ],
      "metadata": {
        "id": "OAVJzs_tAnrE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a GPT model's absolute embedding approach, we just need to create another embedding layer that has the same embedding dimension as the token_embedding_layer."
      ],
      "metadata": {
        "id": "V-bcg7L3ApuW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_length = max_length\n",
        "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
        "pos_embedding = pos_embedding_layer(torch.arange(context_length))\n",
        "print(pos_embedding.shape)"
      ],
      "metadata": {
        "id": "yfL63IRaB3M7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1f0d294-58ee-45ba-ed46-0fd61356c61d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The input to the pos_embeddings is usually a placeholder vector torch.arange(context_length), which contains a sequence of numbers 0, 1, ..., upto the maximum input length - 1. The context_length is a variable that represents the supported input size of the LLM. Here, we choose it similar to the maximum length of the input text. In practice, input text can be longer than the supported context length, in which case we have to truncate the text."
      ],
      "metadata": {
        "id": "yF4ZHDiYCtjP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, the positional embedding tensor consists of four 256-dimensional vectors. We can now add these directly to the token embeddings, where PyTorch will add the 4 x 256-dimensional pos_embeddings tensor to each 4 x 256-dimensional token embedding tensor in each of the eith batches."
      ],
      "metadata": {
        "id": "CvqM69MgDd7V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_embeddings = token_embeddings + pos_embedding\n",
        "print(input_embeddings.shape)"
      ],
      "metadata": {
        "id": "Bv54jkegDc2V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be287a51-0a1f-41f9-9210-9ab096e9a2d9"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ]
    }
  ]
}